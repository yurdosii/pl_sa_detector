# ML service

## To run
### locally:
```
poetry run uvicorn src.main:app --reload
```

### via Docker:
```
docker build -t pl_sa_detector_ml_service .
docker run -p 8000:8000 pl_sa_detector_ml_service
```

---
## Debugging

### To debug in VS Code locally
note: `.vscode/launch.json` is used for this. It was generated by "Run and Debug" -> "Add configuration" -> "Python" -> "FastAPI", after this manually updated to have `--reload`)

Steps:
- open `ml_service` folder in VS Code
- set Python interpreter to `.venv/bin/python`
- (optional) It could be necessary to re-create venv in the new terminal (`poetry shell`, `poetry install`)
- click "Start Debugging" / F5  in the "Run and Debug" section of VS Code

---

### Docker debugging
- set `breakpoint()` somewhere
- run the app like:
```
docker build -t pl_sa_detector_ml_service .
docker run -p 8000:8000 -it --rm pl_sa_detector_ml_service
```

---

### To debug in VS Code in docker container
note: same as in "To debug in VS Code locally" `.vscode/launch.json` is used here, but on the last step I've updated the port to be 8005

Steps:
- open `pl_sa_detector` folder
- click on the green icon (Dev Containers [link](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)) -> "Reopen in Container" -> "from `docker-compose.yml`"
- (optional) It could be necessary to do this couple of times
- open `/code` folder (created on docker container by `Dockerfile` and `docker-compose.yml`)
- (optional) It could be necessary to install Python extension (there will be a pop-up)
- click "Start Debugging" / F5  in the "Run and Debug" section of VS Code

So we ran one docker container (8002 port (docker-compose)) with the server (FastAPI app). Then we connected to that docker container and ran another server on it (8005 port) that we can debug now (set breakpoints in VS Code)
